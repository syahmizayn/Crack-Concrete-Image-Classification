# -*- coding: utf-8 -*-
"""crack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ts2uv-DlNeCjFBLbsJOqg42HiPt3ZM_6

## Install Dependencies
"""

! pip install rarfile

"""## Import Libraries"""

import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))

# Import Neccesary Libraries
import os
import cv2
import imghdr
import requests
import rarfile
import datetime
import numpy as np
import numpy as numpy
from matplotlib import pyplot as plt

# Import TensorFlow Package
import tensorflow as tf
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy
from tensorflow.keras.callbacks import EarlyStopping
import keras.api._v2.keras as keras
from keras.models import Sequential
from  keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout

# URL for the dataset
URL = "https://data.mendeley.com/public-files/datasets/5y9wdsg2zt/files/8a70d8a5-bce9-4291-bab9-b48cfb3e87c3/file_downloaded"

# Define the download path
download_path = "dataset.rar"

# Download the dataset
response = requests.get(URL)
with open(download_path, 'wb') as file:
    file.write(response.content)

"""## Remove Dodgy Images

"""

# Extract the downloaded RAR file
extracted_folder = "dataset"
with rarfile.RarFile(download_path, 'r') as rar_ref:
    rar_ref.extractall(extracted_folder)

# Set The Directory of Dataset
data_dir = 'dataset'

image_exts = ['jpeg', 'jpg', 'bmp', 'png']

for image_class in os.listdir(data_dir):
    for image in os.listdir(os.path.join(data_dir, image_class)):
        image_path = os.path.join(data_dir, image_class, image)
        try:
            img = cv2.imread(image_path)
            tip = imghdr.what(image_path)
            if tip not in image_exts:
                print("Image not in ext list{}".format(image_path))
                os.remove(image_path)
        except Exception as e:
            print('Issue with image {}'.format(image_path))

"""## Load Dataset"""

data = keras.utils.image_dataset_from_directory('dataset')

data_iterator = data.as_numpy_iterator()

batch = data_iterator.next()

fig,ax = plt.subplots(ncols=4, figsize=(20,20))
for idx, img in enumerate(batch[0][:4]):
    ax[idx].imshow(img.astype(int))
    ax[idx].title.set_text(batch[1][idx])

data = data.map(lambda x,y: (x/255,y))

data.as_numpy_iterator().next()

"""## Split The Data"""

train_size = int(len(data)*.7)
val_size = int(len(data)*.2)
test_size = int(len(data)*.1)

train = data.take(train_size)
val = data.skip(train_size).take(val_size)
test = data.skip(train_size+val_size).take(test_size)

"""## Model Training"""

train

model = Sequential()
model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))
model.add(MaxPooling2D())
model.add(Conv2D(32,(3,3),1,activation='relu'))
model.add(MaxPooling2D())
model.add(Conv2D(32,(3,3),1,activation='relu'))
model.add(MaxPooling2D())
model.add(Flatten())
model.add(Dense(256,activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Set early stopping callback
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])

model.summary() # to see the overall structure of our model

logdir = 'logs'
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)

hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback, early_stopping_callback])

"""## TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

"""## Plot Performance"""

fig = plt.figure()
plt.plot(hist.history['loss'], color='teal', label='loss')
plt.plot(hist.history['val_loss'], color='orange', label='val_loss')
fig.suptitle('Loss', fontsize=20)
plt.legend(loc='upper left')
plt.show()

fig = plt.figure()
plt.plot(hist.history['accuracy'], color='teal', label='accuracy')
plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')
fig.suptitle('Accuracy', fontsize=20)
plt.legend(loc='upper left')
plt.show()

"""## Model Evaluation"""

pre = Precision()
re = Recall()
acc = BinaryAccuracy()

for batch in test.as_numpy_iterator():
  x, y = batch
  yhat = model.predict(x)
  pre.update_state(y, yhat)
  re.update_state(y, yhat)
  acc.update_state(y, yhat)

print(pre.result(), re.result(), acc.result())

"""## Test Model"""

img = cv2.imread('/content/dataset/Positive/00008.jpg')
plt.imshow(img)
plt.show()

resize = tf.image.resize(img, (256,256))
plt.imshow(resize.numpy().astype(int))
plt.show

yhat = model.predict(np.expand_dims(resize/255, 0))
yhat

if yhat > 0.5:
  print(f'Predicted class is Crack')
else:
  print(f'Predicted class is Not Crack')

"""## Save Model"""

from tensorflow.keras.models import load_model
tf.keras.models.save_model(model,'crack_model.hdf5')

"""## Deploy Model"""

#installing streamlit
!pip install -q streamlit
!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# import numpy as np
# import cv2
# import tensorflow as tf
# from tensorflow.keras.models import load_model
# 
# # Load your trained model
# model = load_model('crack_model.hdf5')
# 
# # Function to preprocess the image
# def preprocess_image(image):
#     resized_image = tf.image.resize(image, (256, 256))
#     return resized_image.numpy().astype(int) / 255.0
# 
# # Function to make predictions
# def make_prediction(image):
#     preprocessed_image = preprocess_image(image)
#     prediction = model.predict(np.expand_dims(preprocessed_image, 0))
#     return prediction[0][0]
# 
# # Streamlit app
# def main():
#     st.title('Crack Concrete Image Classifier')
#     st.write('Upload an image and the classifier will predict if it contains a crack or not.')
# 
#     # Upload image through Streamlit widget
#     uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])
# 
#     if uploaded_file is not None:
#         # Read the image
#         image = cv2.imdecode(np.fromstring(uploaded_file.read(), np.uint8), 1)
# 
#         # Display the uploaded image
#         st.image(image, channels='BGR', caption='Uploaded Image', use_column_width=True)
# 
#         # Make prediction
#         prediction = make_prediction(image)
#         if prediction > 0.5:
#             st.write('Predicted class is Crack')
#         else:
#             st.write('Predicted class is Not Crack')
# 
# if __name__ == '__main__':
#     main()

import streamlit as st
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.models import load_model

# Load your trained model
model = load_model('crack_model.hdf5')

# Function to preprocess the image
def preprocess_image(image):
    resized_image = tf.image.resize(image, (256, 256))
    return resized_image.numpy().astype(int) / 255.0

# Function to make predictions
def make_prediction(image):
    preprocessed_image = preprocess_image(image)
    prediction = model.predict(np.expand_dims(preprocessed_image, 0))
    return prediction[0][0]

# Streamlit app
def main():
    st.title('Crack Concrete Image Classifier')
    st.write('Upload an image and the classifier will predict if it contains a crack or not.')

    # Upload image through Streamlit widget
    uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

    if uploaded_file is not None:
        # Read the image
        image = cv2.imdecode(np.fromstring(uploaded_file.read(), np.uint8), 1)

        # Display the uploaded image
        st.image(image, channels='BGR', caption='Uploaded Image', use_column_width=True)

        # Make prediction
        prediction = make_prediction(image)
        if prediction > 0.5:
            st.write('Predicted class is Crack')
        else:
            st.write('Predicted class is Not Crack')

if __name__ == '__main__':
    main()

!streamlit run /content/app.py & npx localtunnel -p 8501

!curl ipv4.icanhazip.com